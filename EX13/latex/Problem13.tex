\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{algpseudocode} 

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\begin{document}
\section*{Randomized min-cut algorithm}
Consider the randomized min-cut algorithm discussed in class.
We have seen that its probability of success is at least $1 / \binom{n}{2}$, where $n$ is the number of its vertices.
\begin{itemize}
\item Describe how to implement the algorithm when the graph is represented by adjacency lists, and analyze its running time.
In particular, a contraction step can be done in $O(n)$ time.
\item A weighted graph has a weight $w(e)$ on each edge $e$, which  is  a  positive  real number.
The  min-cut  in  this  case  is  meant  to  be  min-weighted  cut,  where  the sum  of  the  weights  in  the  cut  edges  is  minimum.
Describe  how  to  extend  the algorithm  to  weighted  graphs,  and  show  that  the  probability  of  success  is  still $\geq 1/\binom{n}{2}$. [hint: define the weighted degree of a node]
\item Show that running the algorithm multiple times independently at random, and taking the minimum among the min-cuts thus produced, the probability of success can be made at least $1 - 1/n^c$ or a constant $c > 0$ (hence, with high probability).
\end{itemize}
\
\\
\\
\textbf{SOLUTION}
\\
\\

\noindent
\textbf{First Point}
\\
In order to solve this point, we use two data structure, pre-processed, to have linear access on the data. The graph is saved as adjacency list but, instead saving in a list one entry for each edge, we will save a cell that contains the edge ID and its multiplicity. 

\begin{tikzpicture}
\matrix (M) [matrix of nodes,
column sep=0pt,
row sep=0pt,
nodes={draw,fill=gray!20,minimum width=.5cm,outer sep=0pt,minimum
height=.7cm,anchor=center},
column 1/.style={minimum height=.8cm}]{
 \mbox{} &[2mm] <2, 3> & \mbox{} &[2mm] < \ 5, 13   > & $/$  &[2mm]   &   &[2mm]   & \\
 \mbox{} & <1, 2> & \mbox{} & <id, mul> & \mbox{}  & .. &  \mbox{} & .. & $/$  \\
 \mbox{}  & <.., ..>  & \mbox{} & <id, mul> & $/$  &   &   &   & \\
 \mbox{} & <.., ..> & \mbox{} & <id, mul> & \mbox{}  & .. & $/$  &   & \\
 \mbox{} & <.., ..> & \mbox{} & <id, mul> & \mbox{}  & .. & $/$  &   & \\
};
\foreach \i in {1,2,3,4,5}{
\path (M-\i-1) [late options={label=left:\i}];
\draw[->] (M-\i-1.center)--(M-\i-2.west);
\draw[->] (M-\i-3.center)--(M-\i-4.west);
}
\draw[->] (M-2-5.center)--(M-2-6.west);
\draw[->] (M-4-5.center)--(M-4-6.west);
\draw[->] (M-5-5.center)--(M-5-6.west);
\draw[->] (M-2-7.center)--(M-2-8.west);
\end{tikzpicture}

Another data structure that we will use is a \textit{Virtual Table} $V_T$. $V_T$ is an array of size $n$, that contains for each entry the node that includes the $i^{th}$ node. E.g. The item 4 contains the value 2 ($V_T[4] = 2$): this means that the node 4 is included in the node 2 because the nodes are contracted. Moreover, we assume that the adjacency lists are ordered (if not, then order them!). \\

\noindent
\textit{Random choice of edge}
To choose a edge we will use another data structure, an array $\Delta$ that are initialized as $\Delta[i] = \delta(i)$ (where $\delta(i) =$ the grade of node $i$). Generate, at random, an integer $r \in [0, 2m
]$ and use $r$ to choose the edge: 
\begin{algorithmic}
\State i = 0
\While {$r > 0$}
	\If {$\Delta[i] < r$} 
		\State $r = r - \Delta[i]$
	\Else 
		\State \text{choose} $\Delta[i]$
	\EndIf
	\State $i++$
\EndWhile
\end{algorithmic}
Another, equivalent, approach is to imagine the adjacency lists all concatenated and scroll them until $r > 0$, subtracting from $r$ the multiplicity of the item considered:
\begin{algorithmic}
\State i = 0
\State L = all the adjacency lists concatenated
\While {$r > 0$}
	\If {$L[i].multiplicity < r$} 
		\State $r = r - L[i]$
	\Else 
		\State \text{choose} $L[i]$
	\EndIf
	\State $i++$
\EndWhile
\end{algorithmic}

\noindent
\textit{Merging two adjacency lists}
To merge two nodes we need to scroll the lists of the merged nodes  and, assuming the ordered data, we can merge into one list in $O(2n) = O(n)$ (we use an approach like the merge sort where the resulting multiplicity is the sum of the multiplicity, if exists, of the original lists).
\\
\\

\noindent
\textbf{Second Point}
\\

In the algorithm we change the way we choose an edge to contract: edges with high weight will have a greater probability to be chosen with respect to others with lower weight.
We define $weight: E \longrightarrow \mathbb{R}$ as the function that associate the weight to a given edge.
We will use the $weight$ function also on subsets of $E$, intending for $weight(X)$ with $X \subseteq E$ the sum of the weight of all the edges in $X$.
\begin{align*}
&weight(X) = \sum_{e \in X} weight(e) &where\ X \subseteq E
\end{align*}
As seen in class, the \textit{min cut} is not unique, nevertheless the sum of weights in all \textit{min\_cuts} is the same, than with a notation abuse we will call it $weight(min\_cut)$.
For each edge $x \in E$ the probability to be choose for a contraction is given by its weights normalized with the total sum of the weights of all the edges of the graph.
\begin{align*}
Pr[extract\ x] = \frac{weight(x)}{weight(E)}\\
\end{align*}
Now the probability of making an error, when extracting an edge at random, is the probability of extracting one of the ``bad'' edges, i.e. edges such that their contraction cause a variation in $weight(min\_cut)$.
We name $BAD$ the set of ``bad'' edges.
\begin{align*}
&Pr[error] = \sum_{e \in BAD} Pr[extract\ e] = \sum_{e \in BAD} \frac{weight(e)}{weight(E)} = \frac{weight(BAD)}{weight(E)} \tag{1}
\end{align*}
As usual bad edges are those belonging to every \textit{min cut}, so the total weight of bad edges is less or equal to the weight of \textit{min cut}.
\begin{align*}
&weight(BAD) \leq {weight(min\_cut)} \tag{2}
\end{align*}
Given a node $v$, we define $star(v)$ as the set of all the edges touching $v$.
For the same reasons explained in class, for each node $v$ the weight of \textit{min cut} must be less or equal to the sum of the weights of the edges touching $v$.
This is because otherwise we would have a cut, $star(n)$, with weight less than \textit{min cut}, which is absurd.
\begin{align*}
&\forall\ v \in V\ .\ weight(min\_cut) \leq weight(star(v)) \tag{3}
\end{align*}
We can reformulate the \textit{handshaking lemma} for weighted graph in the following way, in which $weight(star(v))$ is something like the ``weighted degree'' of the node $v$.
\begin{align*}
weight(E) = \frac{\sum_{v \in V} weight(star(v))}{2} \tag{4}
\end{align*}
Now we can derive:
\begin{align*}
weight(E) &= \frac{\sum_{v \in V} weight(star(v))}{2} \tag{from 4}\\
&\geq \frac{\vert V \vert weight(min\_cut)}{2} \tag{from 3}\\ \\
weight(min\_cut) &\leq \frac{2\ weight(E)}{\vert V \vert} \tag{5}
\end{align*}
Finally we have all the ingredients for the proof:
\begin{theorem}
Selecting the edges for the contraction according to the probability described, each time we choose an edge the probability of choosing a ``bad'' edge is less or equal than $2 / \vert V \vert$.
\begin{align*}
Pr[error] \leq \frac{2}{\vert V \vert}
\end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
Pr[error] &= \frac{weight(BAD)}{weight(E)} \tag{from 1}\\
&\leq \frac{weight(min\_cut)}{weight(E)} \tag{from 2}\\
&\leq \frac{\frac{2\ weight(E)}{\vert V \vert}}{weight(E)} \tag{from 5}\\
&= \frac{2\ weight(E)}{\vert V \vert weight(E)} = \frac{2}{\vert V \vert}
\end{align*}
\end{proof}
The probability of making an error when selecting an edge is the same as for the case of a normal graph seen in class, so the probability of success for the algorithm (i.e. the probability of choosing well all the times) is still $\geq 1/ \binom{n}{2}$.
\\
\\
\noindent
\textbf{Third Point}
\\

We know that $P(n) \geq 1 / \binom{n}{2}$ is the probability of choose well for $N$ times.
We have seen also that the probability of error is $\leq (1 - 1/\binom{n}{2})$.
Then the probability of having an error each time in $N$ repetitions of the algorithm is 
\begin{align*}
leq (1 - \frac{1}{\binom{n}{2}})^N \approx e^{-\frac{2N}{n(n-1)}}
\end{align*}

Given a $c$ we want the probability of success to be $\geq 1 - 1/n^c$.
This is equivalent to ask that the probability of error is $\leq 1/n^c$.
Making some calculations:
\begin{align*}
\frac{1}{e^{\frac{2N}{n(n-1)}}} = e^{-\frac{2N}{n(n-1)}} &\leq \frac{1}{n^c}\\
n^c &\leq e^{\frac{2N}{n(n-1)}}\\
ln(n^c) &\leq ln(e^{\frac{2N}{n(n-1)}}) = \frac{2N}{n(n-1)}\\
N \geq \frac{n(n-1)ln(n^c)}{2}
\end{align*}
So, for each given $c$, to make the probability of success at least $1 - 1/n^c$ is sufficient to take $N$ grater or equal than $\frac{1}{2} n(n-1)ln(n^c)$.

\end{document}