\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{algpseudocode} 

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\begin{document}
\section*{Randomized min-cut algorithm}
Consider the randomized min-cut algorithm discussed in class.
We have seen that its probability of success is at least $1 / \binom{n}{2}$, where $n$ is the number of its vertices.
\begin{itemize}
\item Describe how to implement the algorithm when the graph is represented by adjacency lists, and analyze its running time.
In particular, a contraction step can be done in $O(n)$ time.
\item A weighted graph has a weight $w(e)$ on each edge $e$, which  is  a  positive  real number.
The  min-cut  in  this  case  is  meant  to  be  min-weighted  cut,  where  the sum  of  the  weights  in  the  cut  edges  is  minimum.
Describe  how  to  extend  the algorithm  to  weighted  graphs,  and  show  that  the  probability  of  success  is  still $\geq 1/\binom{n}{2}$. [hint: define the weighted degree of a node]
\item Show that running the algorithm multiple times independently at random, and taking the minimum among the min-cuts thus produced, the probability of success can be made at least $1 - 1/n^c$ or a constant $c > 0$ (hence, with high probability).
\end{itemize}
\
\\
\\
\textbf{SOLUTION}
\\
\\

\noindent
\textbf{First Point}

We shall keep nodes in an array. We expect to run $N-2$ contractions, spawning a new node for each of them, thus the array shall be of length $2N-2$, the last $N-2$ elements initially NULL.

Nodes will be annotated with their \emph{degree} and a \emph{new} field with the ID of the node they're currently "contained" in (initially themselves, then the nodes they get contracted to).

An edge $(i,j)$ shall be a vector of three elements $<dest, mult, reverse>$:
\begin{itemize}
\item \emph{dest}: the node ID of the destination (i.e. $j$);
\item \emph{mult}: recall we're in a multigraph, so there can be many edges $(i,j)_k$. We represent them as a single edge annotated with a \emph{multiplicity} value, i.e. $(i,j).mult = |\{(i,j)_k\}|$.

This way, each adjacency list contains $O(N)$ elements;
\item \emph{reverse}: as we're in an undirected graph, for each edge $(i,j)$ there is the reverse $(j,i)$. In $reverse$ we store a pointer to $(j,i)$, as we'll need it for bookkeeping during contraction operations.
\end{itemize}

Adjacency lists shall be doubly linked and sorted on $dest$ fields.
\\
\iffalse
%%%% I'm commenting out the picture because I don't know how to fix it :( %%%%
\begin{tikzpicture}
\matrix (M) [matrix of nodes,
column sep=0pt,
row sep=0pt,
nodes={draw,fill=gray!20,minimum width=.5cm,outer sep=0pt,minimum
height=.7cm,anchor=center},
column 1/.style={minimum height=.8cm}]{
 \mbox{} &[2mm] <2, 3> & \mbox{} &[2mm] < \ 5, 13   > & $/$  &[2mm]   &   &[2mm]   & \\
 \mbox{} & <1, 2> & \mbox{} & <id, mul> & \mbox{}  & .. &  \mbox{} & .. & $/$  \\
 \mbox{}  & <.., ..>  & \mbox{} & <id, mul> & $/$  &   &   &   & \\
 \mbox{} & <.., ..> & \mbox{} & <id, mul> & \mbox{}  & .. & $/$  &   & \\
 \mbox{} & <.., ..> & \mbox{} & <id, mul> & \mbox{}  & .. & $/$  &   & \\
};
\foreach \i in {1,2,3,4,5}{
\path (M-\i-1) [late options={label=left:\i}];
\draw[->] (M-\i-1.center)--(M-\i-2.west);
\draw[->] (M-\i-3.center)--(M-\i-4.west);
}
\draw[->] (M-2-5.center)--(M-2-6.west);
\draw[->] (M-4-5.center)--(M-4-6.west);
\draw[->] (M-5-5.center)--(M-5-6.west);
\draw[->] (M-2-7.center)--(M-2-8.west);
\end{tikzpicture}
\fi


\noindent
\textit{Random choice of edge}

Let $N$ be the initial number of nodes, $n$ be the number of remaining unique nodes.

Let $M$ be the initial number of edges, $m$ be the number of remaining edges.

Let $\delta(i)$ be the degree of node $i$ (NB: contracted nodes will have degree 0). Then:\\

\begin{algorithmic}
\Function{ChooseEdge}{}
\State $r \gets rand(0, 2m-1)$ \Comment{note $\sum_{i=1}^{n}\delta(i) = 2m$}
\State $i \gets 0$ \
\While {$nodes[i].degree < r$} \Comment{note we'll never reach the NULL tail of $nodes[]$}
	\State $r \gets r - nodes[i].degree$
	\State $i \gets i + 1$
\EndWhile
\State $j \gets 0$
\While {$nodes[i].adj[j].mult < r$}
	\State $r \gets r - nodes[i].adj[j].mult$
	\State $j \gets j + 1$
\EndWhile
\State \Return $(i,\ j)$
\State
\EndFunction
\end{algorithmic}

You can think of what we're doing as having concatenated the adjacency lists and then crawled through them to edge $r$. This would require $O(n^2)$ steps done naively, but saving the degree of the node (the length of the adjacency list) allows us to skip to the next node in one step (a sort of skip-pointer).

In fact, what we do is crawl the list of nodes ($O(n)$) and then one adjacency list ($O(n)$ again).
\\

This procedures results in a random uniform choice of the edge. In fact, the chance that a specific (directed!) edge $(i, j)_k$ is chosen ends up being:
$$P_r[(i,j_k)] = \frac{\delta(i)}{2 \times m} \times \frac{mult(i,j)}{\delta(i)} \times \frac{1}{mult(i,j)} = \frac{1}{2 \times m}$$
where
$$\frac{\delta(i)}{2 \times m} = P_r[node\ i\ is\ chosen]$$
$$\frac{mult(i,j)}{\delta(i)} = P_r[\{(i,j)_k\}|\ node\ i]$$
$$\frac{1}{mult(i,j)} = P_r[(i,j)_k|\ \{(i,j)_k\}]$$
$$\{(i,j)_k\}\ \text{is the set of all edges going from i to j}$$
$$mult(i,j) = |\{(i,j)_k\}|\ \text{is the multiplicity of edge (i,j)}$$
\\

Thus, the probability that an (undirected!) edge $(i,j)_k$ is chosen results
$$\frac{1}{2m} + \frac{1}{2m} = \frac{1}{m}$$
\\
\noindent
\textit{Contraction}

General idea for contracting over edge $(i,j)$:
\begin{itemize}
\item Spawn a new node that will represent $i$ and $j$ contracted;
\item Update the \emph{new} field of nodes "contained" in $i$ or $j$;
\item Crawl through $i$ and $j$'s adjacency lists: nodes pointed by one but not the other get appended to the new node's list; nodes pointed by both are also appended, but their multiplicity is the sum of the old egdes' multiplicity.

This can be done in linear time w.r.t. the length of the adjacency lists ($O(N)$) because they're sorted, so we proceed as in a \emph{merge} operation of a MergeSort.

In all cases, we take advantage of $(i,j).reverse$ to update the adjacency list of the pointed node with the new node. As the lists are doubly linked and assuming a pointer to the last element is kept, this can be done in $O(1)$ time.
\end{itemize}

\begin{algorithmic}
\Function{Contract}{i, j}
\State $nID \gets$ position of next NULL element in the tail of $nodes[]$
\State $nodes[n] \gets new\ Node()$
\State $\forall x$ in $nodes \mid x.new = i \lor x.new = j\ .\ x.new \gets nID$
\State $it_i \gets$ iterator\ over\ $nodes[i].adj$
\State $it_j \gets$ iterator\ over\ $nodes[j].adj$
\While{$it_i.hasNext()$ and $it_j.hasNext()$}
	\If{$it_i.dest = j$}
		\State $it_i.next()$
	\ElsIf{$it_j.dest = i$}
		\State $it_j.next()$
	\ElsIf{$it_i.dest = it_j.dest$}
		\State /* create a pair of edges, one the reverse of the other */
		\State $e \gets new\ Edge(it_i.dest, it_i.mult+it_j.mult, new\ Edge(nID, it_i.mult+it_j.mult, NULL))$
		\State $e.reverse \gets e$
		\State /* remove old edges from the destination's adjacency list */
		\State $it_i.reverse.remove()$
		\State $it_j.reverse.remove()$
		\State /* put reverse in destinations; note $nID$ is highest so it goes to the tail*/
		\State $nodes[e.dest].adj.append(e.reverse)$
		\State /* put $e$ at the tail of $nodes[nID].adj$ */
		\State $nodes[nID].adj.append(e)$
		\State $it_i.next()$
		\State $it_j.next()$
		\State /* increment $nodes[nID]$'s degree by $e.mult$ */
		\State $nodes[nID].degree \gets nodes[nID].degree + e.mult$
	\ElsIf{$it_i.dest < it_j.dest$}
		\State $e \gets new\ Edge(it_i.dest, it_i.mult, new\ Edge(nID, it_i.mult, NULL))$
		\State $e.reverse \gets e$
		\State $it_i.reverse.remove()$
		\State $nodes[e.dest].adj.append(e.reverse)$
		\State $nodes[nID].adj.append(e)$
		\State $it_i.next()$
		\State $nodes[nID].degree \gets nodes[nID].degree + e.mult$
	\Else \Comment{$it_j.dest < it_i.dest$}
		\State $e \gets new\ Edge(it_j.dest, it_j.mult, new\ Edge(nID, it_j.mult, NULL))$
		\State $e.reverse \gets e$
		\State $it_j.reverse.remove()$
		\State $nodes[e.dest].adj.append(e.reverse)$
		\State $nodes[nID].adj.append(e)$
		\State $it_j.next()$	
		\State $nodes[nID].degree \gets nodes[nID].degree + e.mult$
	\EndIf
\EndWhile
\State Process leftovers from either $i$ or $j$'s adjacency lists, if there are any, in a similar fashion.
\State $nodes[i].degree \gets 0$ \Comment{degrees of $i$ and $j$ are put to 0 for ChooseEdge's conveniency}
\State $nodes[j].degree \gets 0$ \Comment{we could also detach the adjacency lists if we liked, but it's not necessary}
\EndFunction
\end{algorithmic}
\hfill

\noindent
\textit{Computing the cut}

After running $N-2$ contractions, you'll end up with two nodes $hl_1$ and $hl_2$ (highlanders) left over in the graph.

The size of the cut will be $hl_1.degree = hl_2.degree$.

The \emph{new} field of the first $N$ elements of $nodes$ will be either $hl_1$ or $hl_2$. 

The two classes will thus define the cut. Should you need the actual edges making up the cut, you'll crawl through the original graph and output edges connecting nodes of the two classes ($O(N^2)$).

\break
\noindent
\textbf{Second Point}
\\

In the algorithm we change the way we choose an edge to contract: edges with high weight will have a greater probability to be chosen with respect to others with lower weight.
We define $weight: E \longrightarrow \mathbb{R}$ as the function that associate the weight to a given edge.
We will use the $weight$ function also on subsets of $E$, intending for $weight(X)$ with $X \subseteq E$ the sum of the weight of all the edges in $X$.
\begin{align*}
&weight(X) = \sum_{e \in X} weight(e) &where\ X \subseteq E
\end{align*}
As seen in class, the \textit{min cut} is not unique, nevertheless the sum of weights in all \textit{min\_cuts} is the same, than with a notation abuse we will call it $weight(min\_cut)$.
For each edge $x \in E$ the probability to be choose for a contraction is given by its weights normalized with the total sum of the weights of all the edges of the graph.
\begin{align*}
Pr[extract\ x] = \frac{weight(x)}{weight(E)}\\
\end{align*}
Now the probability of making an error, when extracting an edge at random, is the probability of extracting one of the ``bad'' edges, i.e. edges such that their contraction cause a variation in $weight(min\_cut)$.
We name $BAD$ the set of ``bad'' edges.
\begin{align*}
&Pr[error] = \sum_{e \in BAD} Pr[extract\ e] = \sum_{e \in BAD} \frac{weight(e)}{weight(E)} = \frac{weight(BAD)}{weight(E)} \tag{1}
\end{align*}
As usual bad edges are those belonging to every \textit{min cut}, so the total weight of bad edges is less or equal to the weight of \textit{min cut}.
\begin{align*}
&weight(BAD) \leq {weight(min\_cut)} \tag{2}
\end{align*}
Given a node $v$, we define $star(v)$ as the set of all the edges touching $v$.
For the same reasons explained in class, for each node $v$ the weight of \textit{min cut} must be less or equal to the sum of the weights of the edges touching $v$.
This is because otherwise we would have a cut, $star(n)$, with weight less than \textit{min cut}, which is absurd.
\begin{align*}
&\forall\ v \in V\ .\ weight(min\_cut) \leq weight(star(v)) \tag{3}
\end{align*}
We can reformulate the \textit{handshaking lemma} for weighted graph in the following way, in which $weight(star(v))$ is something like the ``weighted degree'' of the node $v$.
\begin{align*}
weight(E) = \frac{\sum_{v \in V} weight(star(v))}{2} \tag{4}
\end{align*}
Now we can derive:
\begin{align*}
weight(E) &= \frac{\sum_{v \in V} weight(star(v))}{2} \tag{from 4}\\
&\geq \frac{\vert V \vert weight(min\_cut)}{2} \tag{from 3}\\ \\
weight(min\_cut) &\leq \frac{2\ weight(E)}{\vert V \vert} \tag{5}
\end{align*}
Finally we have all the ingredients for the proof:
\begin{theorem}
Selecting the edges for the contraction according to the probability described, each time we choose an edge the probability of choosing a ``bad'' edge is less or equal than $2 / \vert V \vert$.
\begin{align*}
Pr[error] \leq \frac{2}{\vert V \vert}
\end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
Pr[error] &= \frac{weight(BAD)}{weight(E)} \tag{from 1}\\
&\leq \frac{weight(min\_cut)}{weight(E)} \tag{from 2}\\
&\leq \frac{\frac{2\ weight(E)}{\vert V \vert}}{weight(E)} \tag{from 5}\\
&= \frac{2\ weight(E)}{\vert V \vert weight(E)} = \frac{2}{\vert V \vert}
\end{align*}
\end{proof}
The probability of making an error when selecting an edge is the same as for the case of a normal graph seen in class, so the probability of success for the algorithm (i.e. the probability of choosing well all the times) is still $\geq 1/ \binom{n}{2}$.
\\
\\
\noindent
\textbf{Third Point}
\\

We know that $P(n) \geq 1 / \binom{n}{2}$ is the probability of choose well for $N$ times.
We have seen also that the probability of error is $\leq (1 - 1/\binom{n}{2})$.
Then the probability of having an error each time in $N$ repetitions of the algorithm is 
\begin{align*}
leq (1 - \frac{1}{\binom{n}{2}})^N \approx e^{-\frac{2N}{n(n-1)}}
\end{align*}

Given a $c$ we want the probability of success to be $\geq 1 - 1/n^c$.
This is equivalent to ask that the probability of error is $\leq 1/n^c$.
Making some calculations:
\begin{align*}
\frac{1}{e^{\frac{2N}{n(n-1)}}} = e^{-\frac{2N}{n(n-1)}} &\leq \frac{1}{n^c}\\
n^c &\leq e^{\frac{2N}{n(n-1)}}\\
ln(n^c) &\leq ln(e^{\frac{2N}{n(n-1)}}) = \frac{2N}{n(n-1)}\\
N \geq \frac{n(n-1)ln(n^c)}{2}
\end{align*}
So, for each given $c$, to make the probability of success at least $1 - 1/n^c$ is sufficient to take $N$ grater or equal than $\frac{1}{2} n(n-1)ln(n^c)$.

\end{document}